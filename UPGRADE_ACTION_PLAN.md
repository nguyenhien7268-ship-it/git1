# Káº¿ Hoáº¡ch HÃ nh Äá»™ng NÃ¢ng Cáº¥p Há»‡ Thá»‘ng (V7.3 â†’ V8.0)

## ðŸ“‹ Executive Summary

Dá»±a trÃªn Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n trong file `SYSTEM_EVALUATION.md`, Ä‘Ã¢y lÃ  káº¿ hoáº¡ch hÃ nh Ä‘á»™ng cá»¥ thá»ƒ Ä‘á»ƒ nÃ¢ng cáº¥p há»‡ thá»‘ng tá»« V7.3 lÃªn V8.0.

**Thá»i gian dá»± kiáº¿n:** 2-3 thÃ¡ng  
**Äá»™ Æ°u tiÃªn:** Cao â†’ Trung bÃ¬nh â†’ Tháº¥p  
**Má»¥c tiÃªu chÃ­nh:** TÄƒng stability, performance, vÃ  chuáº©n bá»‹ cho scale

---

## ðŸŽ¯ Week 1-2: Quick Wins (Cáº£i thiá»‡n nhanh)

### âœ… Task 1: Setup Testing Infrastructure
**Thá»i gian:** 5 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** Dev Team  
**Äá»™ Æ°u tiÃªn:** ðŸ”´ Cao

**Chi tiáº¿t:**
```bash
# 1. Setup pytest vá»›i coverage
pip install pytest pytest-cov pytest-mock

# 2. Táº¡o cáº¥u trÃºc test
mkdir -p tests/{unit,integration}
touch tests/unit/test_ml_model.py
touch tests/unit/test_backtester.py
touch tests/unit/test_ai_feature_extractor.py
touch tests/integration/test_mvp_flow.py

# 3. Viáº¿t test cho 5 modules quan trá»ng nháº¥t
# - logic/ml_model.py
# - logic/backtester.py
# - logic/ai_feature_extractor.py
# - logic/dashboard_analytics.py
# - logic/data_repository.py

# 4. Target: 50% coverage trong tuáº§n Ä‘áº§u, 70% cuá»‘i tuáº§n 2
pytest --cov=logic --cov-report=html
```

**Checklist:**
- [ ] Setup pytest vÃ  pytest-cov
- [ ] Táº¡o test fixtures cho data máº«u
- [ ] Viáº¿t 20+ unit tests cho ml_model.py
- [ ] Viáº¿t 15+ unit tests cho backtester.py
- [ ] Viáº¿t 10+ unit tests cho ai_feature_extractor.py
- [ ] Viáº¿t 5+ integration tests cho MVP flow
- [ ] Äáº¡t 70% code coverage
- [ ] Setup GitHub Actions Ä‘á»ƒ run tests tá»± Ä‘á»™ng

**Deliverables:**
- [ ] `tests/` folder vá»›i 50+ tests
- [ ] Coverage report > 70%
- [ ] CI pipeline running tests

---

### âœ… Task 2: Implement Caching
**Thá»i gian:** 3 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** Backend Dev  
**Äá»™ Æ°u tiÃªn:** ðŸ”´ Cao

**Chi tiáº¿t:**
```python
# 1. ThÃªm caching cho cÃ¡c hÃ m pure
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_loto_stats_last_n_days(data_tuple, days):
    # Convert list to tuple Ä‘á»ƒ cÃ³ thá»ƒ cache
    # Implement logic
    pass

# 2. Cache daily_bridge_predictions
import pickle
import os
from datetime import datetime

CACHE_DIR = "cache/"
CACHE_FILE = f"{CACHE_DIR}daily_predictions_{datetime.now().date()}.pkl"

def get_daily_predictions_with_cache(all_data_ai):
    # Check cache
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'rb') as f:
            return pickle.load(f)
    
    # Compute
    result = _get_daily_bridge_predictions(all_data_ai)
    
    # Save cache
    os.makedirs(CACHE_DIR, exist_ok=True)
    with open(CACHE_FILE, 'wb') as f:
        pickle.dump(result, f)
    
    return result

# 3. Cache AI predictions
@lru_cache(maxsize=100)
def get_ai_predictions_cached(data_hash, model_version):
    # Implement logic
    pass
```

**Checklist:**
- [ ] Add `@lru_cache` cho cÃ¡c hÃ m pure trong `dashboard_analytics.py`
- [ ] Implement disk cache cho `daily_bridge_predictions`
- [ ] Cache AI predictions vá»›i TTL = 1 ngÃ y
- [ ] ThÃªm cache invalidation logic
- [ ] Benchmark performance (trÆ°á»›c vÃ  sau)

**Deliverables:**
- [ ] Cache implementation trong `logic/cache_manager.py`
- [ ] Performance improvement > 30%
- [ ] Documentation vá» caching strategy

---

### âœ… Task 3: Code Quality Tools
**Thá»i gian:** 2 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** All Devs  
**Äá»™ Æ°u tiÃªn:** ðŸŸ¡ Trung bÃ¬nh

**Chi tiáº¿t:**
```bash
# 1. Install tools
pip install black flake8 isort mypy

# 2. Create config files
# .flake8
cat > .flake8 << 'EOF'
[flake8]
max-line-length = 100
exclude = .git,__pycache__,build,dist
ignore = E203,W503
EOF

# pyproject.toml
cat > pyproject.toml << 'EOF'
[tool.black]
line-length = 100
target-version = ['py312']

[tool.isort]
profile = "black"
line_length = 100

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
EOF

# 3. Run formatters
black .
isort .
flake8 .

# 4. Add pre-commit hooks
pip install pre-commit
cat > .pre-commit-config.yaml << 'EOF'
repos:
  - repo: https://github.com/psf/black
    rev: 24.1.1
    hooks:
      - id: black
  - repo: https://github.com/PyCQA/isort
    rev: 5.13.2
    hooks:
      - id: isort
  - repo: https://github.com/PyCQA/flake8
    rev: 7.0.0
    hooks:
      - id: flake8
EOF
pre-commit install
```

**Checklist:**
- [ ] Setup black, flake8, isort
- [ ] Format táº¥t cáº£ files
- [ ] Fix táº¥t cáº£ flake8 warnings
- [ ] Setup pre-commit hooks
- [ ] Update CI Ä‘á»ƒ check formatting

**Deliverables:**
- [ ] All code formatted vá»›i black
- [ ] Zero flake8 warnings
- [ ] Pre-commit hooks working

---

### âœ… Task 4: Add Type Hints & Docstrings
**Thá»i gian:** 3 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** All Devs  
**Äá»™ Æ°u tiÃªn:** ðŸŸ¡ Trung bÃ¬nh

**Chi tiáº¿t:**
```python
# Example: ThÃªm type hints vÃ  docstrings
from typing import List, Dict, Tuple, Optional
import numpy as np

def prepare_training_data(
    all_data_ai: List[List[Any]], 
    daily_bridge_predictions: Dict[str, Dict[str, Dict[str, float]]]
) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
    """
    Chuáº©n bá»‹ dá»¯ liá»‡u huáº¥n luyá»‡n cho mÃ´ hÃ¬nh AI XGBoost.
    
    Args:
        all_data_ai: Danh sÃ¡ch dá»¯ liá»‡u A:I tá»« database, má»—i pháº§n tá»­ lÃ  list 10 cá»™t
                     [MaSoKy, Col_A_Ky, Col_B_GDB, ..., Col_I_G7]
        daily_bridge_predictions: Dictionary chá»©a dá»± Ä‘oÃ¡n cáº§u cho má»—i ngÃ y
                                  Format: {ky: {loto: {feature_name: value}}}
    
    Returns:
        Tuple[X, y] náº¿u thÃ nh cÃ´ng, (None, None) náº¿u tháº¥t báº¡i
        - X: Features matrix (n_samples, n_features)
        - y: Labels array (n_samples,)
    
    Raises:
        ValueError: Náº¿u all_data_ai cÃ³ Ã­t hÆ¡n MIN_DATA_TO_TRAIN samples
    
    Example:
        >>> X, y = prepare_training_data(data, predictions)
        >>> if X is not None:
        ...     model.fit(X, y)
    """
    if not all_data_ai or len(all_data_ai) < MIN_DATA_TO_TRAIN:
        raise ValueError(f"Cáº§n tá»‘i thiá»ƒu {MIN_DATA_TO_TRAIN} ká»³ Ä‘á»ƒ huáº¥n luyá»‡n AI.")
    # ... rest of implementation
```

**Checklist:**
- [ ] Add type hints cho táº¥t cáº£ public functions
- [ ] Add docstrings (Google Style) cho táº¥t cáº£ public functions
- [ ] Run mypy vÃ  fix errors
- [ ] Generate documentation vá»›i Sphinx

**Deliverables:**
- [ ] 100% functions cÃ³ type hints
- [ ] 100% public functions cÃ³ docstrings
- [ ] Sphinx documentation generated

---

## ðŸ”§ Week 3-4: Architecture Refactoring

### âœ… Task 5: Loáº¡i Bá» Phá»¥ Thuá»™c ChÃ©o
**Thá»i gian:** 1 tuáº§n  
**NgÆ°á»i thá»±c hiá»‡n:** Senior Dev  
**Äá»™ Æ°u tiÃªn:** ðŸ”´ Cao

**Chi tiáº¿t:**

**Váº¥n Ä‘á» hiá»‡n táº¡i:**
```python
# ml_model.py (HIá»†N Táº I - SAI)
from .bridges.bridges_classic import getAllLoto_V30  # âŒ Import trá»±c tiáº¿p
from .dashboard_analytics import get_loto_stats_last_n_days  # âŒ Import trá»±c tiáº¿p

def prepare_training_data(all_data_ai, daily_bridge_predictions):
    # Sá»­ dá»¥ng trá»±c tiáº¿p cÃ¡c hÃ m nÃ y
    current_loto_results = set(getAllLoto_V30(current_data))
    loto_stats = get_loto_stats_last_n_days(all_data_ai[:i], stats_days)
```

**Giáº£i phÃ¡p:**
```python
# lottery_service.py hoáº·c ai_feature_extractor.py (Má»šI - ÄÃšNG)
def prepare_features_for_ml(all_data_ai):
    """
    Thu tháº­p táº¥t cáº£ features cáº§n thiáº¿t cho ML model.
    """
    features_by_ky = {}
    
    for i in range(1, len(all_data_ai)):
        current_data = all_data_ai[i]
        current_ky = str(current_data[0])
        
        # TÃ­nh toÃ¡n táº¥t cáº£ features táº¡i Ä‘Ã¢y
        current_loto_results = set(getAllLoto_V30(current_data))
        loto_stats = get_loto_stats_last_n_days(all_data_ai[:i], stats_days)
        loto_gan_stats = get_loto_gan_stats(all_data_ai[:i], gan_max_days)
        
        # ÄÃ³ng gÃ³i thÃ nh dictionary
        features_by_ky[current_ky] = {
            'loto_results': current_loto_results,
            'loto_stats': loto_stats,
            'loto_gan_stats': loto_gan_stats,
            # ... more features
        }
    
    return features_by_ky

# ml_model.py (Má»šI - ÄÃšNG)
def prepare_training_data(
    all_data_ai: List[List[Any]], 
    daily_bridge_predictions: Dict,
    precomputed_features: Dict  # âœ… Nháº­n features tá»« bÃªn ngoÃ i
) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
    """
    Model chá»‰ nháº­n data Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹ sáºµn, khÃ´ng tá»± tÃ­nh toÃ¡n.
    """
    X = []
    y = []
    
    for ky, features in precomputed_features.items():
        # Sá»­ dá»¥ng features Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh sáºµn
        loto_results = features['loto_results']
        loto_stats = features['loto_stats']
        
        # Build X, y
        # ...
    
    return np.array(X), np.array(y)
```

**Checklist:**
- [ ] Táº¡o function `prepare_features_for_ml()` trong `ai_feature_extractor.py`
- [ ] Refactor `ml_model.py` Ä‘á»ƒ nháº­n precomputed features
- [ ] XÃ³a import `bridges_classic` vÃ  `dashboard_analytics` khá»i `ml_model.py`
- [ ] Update `lottery_service.py` Ä‘á»ƒ gá»i Ä‘Ãºng flow
- [ ] Run tests Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng break
- [ ] Benchmark performance

**Deliverables:**
- [ ] `ml_model.py` khÃ´ng cÃ²n import bridge hoáº·c analytics modules
- [ ] All tests pass
- [ ] Documentation updated

---

### âœ… Task 6: Implement Dependency Injection
**Thá»i gian:** 3 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** Senior Dev  
**Äá»™ Æ°u tiÃªn:** ðŸŸ¡ Trung bÃ¬nh

**Chi tiáº¿t:**
```python
# HIá»†N Táº I (SAI)
DB_NAME = 'data/xo_so_prizes_all_logic.db'  # âŒ Hardcode

def load_data_ai_from_db():
    conn = sqlite3.connect(DB_NAME)  # âŒ Dependency cá»©ng
    # ...

# Má»šI (ÄÃšNG) - Dependency Injection
class DatabaseConnection:
    """Interface cho database connection."""
    def execute(self, query: str) -> List[Any]:
        raise NotImplementedError

class SQLiteConnection(DatabaseConnection):
    """Concrete implementation cho SQLite."""
    def __init__(self, db_path: str):
        self.db_path = db_path
    
    def execute(self, query: str) -> List[Any]:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute(query)
        result = cursor.fetchall()
        conn.close()
        return result

class DataRepository:
    """Repository nháº­n DB connection qua DI."""
    def __init__(self, db_connection: DatabaseConnection):
        self.db = db_connection  # âœ… Inject dependency
    
    def load_data_ai(self) -> List[List[Any]]:
        return self.db.execute('''
            SELECT MaSoKy, Col_A_Ky, ...
            FROM DuLieu_AI
            ORDER BY MaSoKy ASC
        ''')

# Usage
db_conn = SQLiteConnection('data/xo_so_prizes_all_logic.db')
repo = DataRepository(db_conn)
data = repo.load_data_ai()

# Testing
mock_db = MockDatabaseConnection()  # âœ… Dá»… dÃ ng mock
test_repo = DataRepository(mock_db)
```

**Checklist:**
- [ ] Táº¡o `DatabaseConnection` interface
- [ ] Implement `SQLiteConnection` class
- [ ] Refactor `DataRepository` Ä‘á»ƒ nháº­n connection qua constructor
- [ ] Update `MLModel` Ä‘á»ƒ nháº­n repository qua constructor
- [ ] Update all call sites
- [ ] Write tests vá»›i mock dependencies

**Deliverables:**
- [ ] All modules dÃ¹ng DI
- [ ] Test coverage tÄƒng lÃªn (dá»… mock hÆ¡n)
- [ ] Documentation updated

---

### âœ… Task 7: Optimize Database Queries
**Thá»i gian:** 2 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** Backend Dev  
**Äá»™ Æ°u tiÃªn:** ðŸŸ¡ Trung bÃ¬nh

**Chi tiáº¿t:**
```python
# HIá»†N Táº I (SAI)
cursor.execute('SELECT * FROM ManagedBridges')  # âŒ SELECT *

# Má»šI (ÄÃšNG)
cursor.execute('''
    SELECT id, bridge_name, win_rate_text, max_lose_streak_k2n, is_enabled
    FROM ManagedBridges
    WHERE is_enabled = 1
    ORDER BY id DESC
    LIMIT 100
''')  # âœ… Chá»‰ láº¥y cá»™t cáº§n thiáº¿t, cÃ³ WHERE, LIMIT

# ThÃªm indexes
cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_managed_bridges_enabled 
    ON ManagedBridges(is_enabled)
''')

cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_dulieu_ai_maso_ky 
    ON DuLieu_AI(MaSoKy)
''')
```

**Checklist:**
- [ ] Review táº¥t cáº£ SQL queries
- [ ] Replace `SELECT *` vá»›i explicit columns
- [ ] Add WHERE clauses where possible
- [ ] Add LIMIT for large queries
- [ ] Create indexes cho cÃ¡c cá»™t thÆ°á»ng query
- [ ] Benchmark query performance

**Deliverables:**
- [ ] All queries optimized
- [ ] Indexes created
- [ ] Query time giáº£m 50%+

---

## âš™ï¸ Week 5-6: AI & Monitoring

### âœ… Task 8: Model Versioning & Metadata
**Thá»i gian:** 2 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** ML Engineer  
**Äá»™ Æ°u tiÃªn:** ðŸ”´ Cao

**Chi tiáº¿t:**
```python
# Cáº¥u trÃºc file model má»›i
logic/ml_model_files/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ loto_model_v7.3.0.joblib
â”‚   â”œâ”€â”€ loto_model_v7.3.1.joblib
â”‚   â””â”€â”€ loto_model_v8.0.0.joblib
â”œâ”€â”€ scalers/
â”‚   â”œâ”€â”€ ai_scaler_v7.3.0.joblib
â”‚   â””â”€â”€ ai_scaler_v8.0.0.joblib
â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ model_v7.3.0_metadata.json
â”‚   â””â”€â”€ model_v8.0.0_metadata.json
â””â”€â”€ active_model.txt  # Chá»©a version Ä‘ang active

# metadata.json structure
{
    "model_version": "8.0.0",
    "trained_date": "2026-01-15T10:30:00",
    "training_samples": 5000,
    "features": [
        "loto_hot_freq",
        "loto_gan_freq",
        "bridge_count",
        "avg_win_rate",
        "min_k2n_risk",
        "current_lose_streak"
    ],
    "hyperparameters": {
        "max_depth": 6,
        "n_estimators": 200,
        "learning_rate": 0.05,
        "objective": "binary:logistic"
    },
    "metrics": {
        "train_accuracy": 0.82,
        "val_accuracy": 0.78,
        "test_accuracy": 0.76,
        "precision": 0.74,
        "recall": 0.79,
        "f1_score": 0.76
    },
    "data_range": {
        "start_ky": "23001",
        "end_ky": "25364"
    }
}

# Code implementation
import json
from datetime import datetime
from pathlib import Path

class ModelVersionManager:
    def __init__(self, model_dir: Path = Path("logic/ml_model_files")):
        self.model_dir = model_dir
        self.models_dir = model_dir / "models"
        self.scalers_dir = model_dir / "scalers"
        self.metadata_dir = model_dir / "metadata"
        
        # Create dirs
        for d in [self.models_dir, self.scalers_dir, self.metadata_dir]:
            d.mkdir(parents=True, exist_ok=True)
    
    def save_model(
        self, 
        model, 
        scaler, 
        version: str,
        metadata: dict
    ):
        """Save model vá»›i version vÃ  metadata."""
        # Save model
        model_path = self.models_dir / f"loto_model_v{version}.joblib"
        joblib.dump(model, model_path)
        
        # Save scaler
        scaler_path = self.scalers_dir / f"ai_scaler_v{version}.joblib"
        joblib.dump(scaler, scaler_path)
        
        # Save metadata
        metadata_path = self.metadata_dir / f"model_v{version}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # Update active model
        active_path = self.model_dir / "active_model.txt"
        active_path.write_text(version)
        
        print(f"âœ… ÄÃ£ lÆ°u model version {version}")
    
    def load_model(self, version: str = None):
        """Load model theo version. Náº¿u khÃ´ng chá»‰ Ä‘á»‹nh, load active model."""
        if version is None:
            active_path = self.model_dir / "active_model.txt"
            if active_path.exists():
                version = active_path.read_text().strip()
            else:
                raise ValueError("KhÃ´ng tÃ¬m tháº¥y active model")
        
        model_path = self.models_dir / f"loto_model_v{version}.joblib"
        scaler_path = self.scalers_dir / f"ai_scaler_v{version}.joblib"
        metadata_path = self.metadata_dir / f"model_v{version}_metadata.json"
        
        model = joblib.load(model_path)
        scaler = joblib.load(scaler_path)
        
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        return model, scaler, metadata
    
    def list_models(self):
        """List táº¥t cáº£ models Ä‘Ã£ lÆ°u."""
        models = list(self.models_dir.glob("loto_model_v*.joblib"))
        versions = [m.stem.replace("loto_model_v", "") for m in models]
        
        result = []
        for version in sorted(versions, reverse=True):
            metadata_path = self.metadata_dir / f"model_v{version}_metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                result.append({
                    'version': version,
                    'trained_date': metadata['trained_date'],
                    'test_accuracy': metadata['metrics']['test_accuracy']
                })
        
        return result
```

**Checklist:**
- [ ] Implement `ModelVersionManager` class
- [ ] Update `train_ai_model()` Ä‘á»ƒ lÆ°u metadata
- [ ] Update `get_ai_predictions()` Ä‘á»ƒ load tá»« ModelVersionManager
- [ ] ThÃªm UI Ä‘á»ƒ xem/switch giá»¯a cÃ¡c model versions
- [ ] Write tests cho ModelVersionManager

**Deliverables:**
- [ ] ModelVersionManager working
- [ ] All models cÃ³ metadata
- [ ] UI Ä‘á»ƒ manage models

---

### âœ… Task 9: Model Monitoring & Logging
**Thá»i gian:** 3 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** ML Engineer  
**Äá»™ Æ°u tiÃªn:** ðŸ”´ Cao

**Chi tiáº¿t:**
```python
# Táº¡o báº£ng database Ä‘á»ƒ log predictions
CREATE TABLE IF NOT EXISTS AI_Predictions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    prediction_date DATETIME DEFAULT CURRENT_TIMESTAMP,
    model_version TEXT NOT NULL,
    ky TEXT NOT NULL,
    loto TEXT NOT NULL,
    predicted_probability REAL NOT NULL,
    predicted_label INTEGER NOT NULL,  -- 0 hoáº·c 1
    actual_result INTEGER,  -- NULL náº¿u chÆ°a biáº¿t, 0/1 sau khi cÃ³ káº¿t quáº£
    is_correct BOOLEAN,  -- NULL náº¿u chÆ°a biáº¿t
    features_json TEXT  -- LÆ°u features Ä‘á»ƒ debug
);

CREATE TABLE IF NOT EXISTS Model_Metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    calculation_date DATETIME DEFAULT CURRENT_TIMESTAMP,
    model_version TEXT NOT NULL,
    date_range_start TEXT,
    date_range_end TEXT,
    total_predictions INTEGER,
    accuracy REAL,
    precision_score REAL,
    recall REAL,
    f1_score REAL
);

# Implementation
import sqlite3
import json
from datetime import datetime, timedelta

class ModelMonitor:
    def __init__(self, db_path: str = "data/xo_so_prizes_all_logic.db"):
        self.db_path = db_path
        self._create_tables()
    
    def _create_tables(self):
        """Táº¡o báº£ng náº¿u chÆ°a tá»“n táº¡i."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS AI_Predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                prediction_date DATETIME DEFAULT CURRENT_TIMESTAMP,
                model_version TEXT NOT NULL,
                ky TEXT NOT NULL,
                loto TEXT NOT NULL,
                predicted_probability REAL NOT NULL,
                predicted_label INTEGER NOT NULL,
                actual_result INTEGER,
                is_correct BOOLEAN,
                features_json TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS Model_Metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                calculation_date DATETIME DEFAULT CURRENT_TIMESTAMP,
                model_version TEXT NOT NULL,
                date_range_start TEXT,
                date_range_end TEXT,
                total_predictions INTEGER,
                accuracy REAL,
                precision_score REAL,
                recall REAL,
                f1_score REAL
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def log_prediction(
        self,
        model_version: str,
        ky: str,
        loto: str,
        probability: float,
        label: int,
        features: dict = None
    ):
        """Log má»™t prediction."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        features_json = json.dumps(features) if features else None
        
        cursor.execute('''
            INSERT INTO AI_Predictions 
            (model_version, ky, loto, predicted_probability, predicted_label, features_json)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (model_version, ky, loto, probability, label, features_json))
        
        conn.commit()
        conn.close()
    
    def update_actual_result(self, ky: str, loto: str, actual_result: int):
        """Update káº¿t quáº£ thá»±c táº¿ sau khi cÃ³ káº¿t quáº£."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE AI_Predictions
            SET actual_result = ?,
                is_correct = (predicted_label = ?)
            WHERE ky = ? AND loto = ?
        ''', (actual_result, actual_result, ky, loto))
        
        conn.commit()
        conn.close()
    
    def calculate_metrics(self, days: int = 30) -> dict:
        """TÃ­nh metrics cho N ngÃ y gáº§n nháº¥t."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Get current model version
        model_version = self._get_current_model_version()
        
        # Query predictions trong N ngÃ y gáº§n nháº¥t
        cursor.execute('''
            SELECT predicted_label, actual_result, is_correct
            FROM AI_Predictions
            WHERE model_version = ?
                AND actual_result IS NOT NULL
                AND prediction_date >= datetime('now', '-{} days')
        '''.format(days), (model_version,))
        
        rows = cursor.fetchall()
        conn.close()
        
        if not rows:
            return None
        
        # Calculate metrics
        total = len(rows)
        correct = sum(1 for _, _, is_correct in rows if is_correct)
        accuracy = correct / total if total > 0 else 0
        
        # Precision, Recall, F1
        tp = sum(1 for pred, actual, _ in rows if pred == 1 and actual == 1)
        fp = sum(1 for pred, actual, _ in rows if pred == 1 and actual == 0)
        fn = sum(1 for pred, actual, _ in rows if pred == 0 and actual == 1)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        metrics = {
            'model_version': model_version,
            'total_predictions': total,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }
        
        # Save to database
        self._save_metrics(metrics, days)
        
        return metrics
    
    def _save_metrics(self, metrics: dict, days: int):
        """LÆ°u metrics vÃ o database."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        date_end = datetime.now().strftime('%Y-%m-%d')
        date_start = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        cursor.execute('''
            INSERT INTO Model_Metrics
            (model_version, date_range_start, date_range_end, 
             total_predictions, accuracy, precision_score, recall, f1_score)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            metrics['model_version'],
            date_start,
            date_end,
            metrics['total_predictions'],
            metrics['accuracy'],
            metrics['precision'],
            metrics['recall'],
            metrics['f1_score']
        ))
        
        conn.commit()
        conn.close()
    
    def get_metrics_history(self, limit: int = 30) -> list:
        """Láº¥y lá»‹ch sá»­ metrics."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT calculation_date, model_version, 
                   total_predictions, accuracy, precision_score, recall, f1_score
            FROM Model_Metrics
            ORDER BY calculation_date DESC
            LIMIT ?
        ''', (limit,))
        
        rows = cursor.fetchall()
        conn.close()
        
        return [
            {
                'date': row[0],
                'model_version': row[1],
                'total_predictions': row[2],
                'accuracy': row[3],
                'precision': row[4],
                'recall': row[5],
                'f1_score': row[6]
            }
            for row in rows
        ]
    
    def _get_current_model_version(self) -> str:
        """Get current model version."""
        from pathlib import Path
        active_path = Path("logic/ml_model_files/active_model.txt")
        if active_path.exists():
            return active_path.read_text().strip()
        return "unknown"
```

**Checklist:**
- [ ] Implement `ModelMonitor` class
- [ ] Update `get_ai_predictions()` Ä‘á»ƒ log predictions
- [ ] Táº¡o background task Ä‘á»ƒ update actual results
- [ ] Táº¡o background task Ä‘á»ƒ calculate metrics hÃ ng ngÃ y
- [ ] ThÃªm UI dashboard Ä‘á»ƒ hiá»ƒn thá»‹ metrics
- [ ] Setup alert khi accuracy < threshold

**Deliverables:**
- [ ] ModelMonitor working
- [ ] Predictions Ä‘Æ°á»£c log
- [ ] Daily metrics calculation
- [ ] Metrics dashboard trong UI

---

### âœ… Task 10: Hyperparameter Tuning vá»›i Optuna
**Thá»i gian:** 4 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** ML Engineer  
**Äá»™ Æ°u tiÃªn:** ðŸŸ¡ Trung bÃ¬nh

**Chi tiáº¿t:**
```bash
# Install Optuna
pip install optuna

# Implement
```

```python
import optuna
from sklearn.model_selection import cross_val_score
import xgboost as xgb

class HyperparameterTuner:
    def __init__(self, X, y):
        self.X = X
        self.y = y
        self.best_params = None
        self.best_score = None
    
    def objective(self, trial):
        """Objective function cho Optuna."""
        # Define hyperparameters to tune
        params = {
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'n_estimators': trial.suggest_int('n_estimators', 100, 500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'gamma': trial.suggest_float('gamma', 0, 5),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'random_state': 42
        }
        
        # Create model
        model = xgb.XGBClassifier(**params)
        
        # Cross-validation
        scores = cross_val_score(
            model, self.X, self.y, 
            cv=5, 
            scoring='accuracy',
            n_jobs=-1
        )
        
        return scores.mean()
    
    def tune(self, n_trials: int = 100):
        """Cháº¡y hyperparameter tuning."""
        study = optuna.create_study(direction='maximize')
        study.optimize(self.objective, n_trials=n_trials, show_progress_bar=True)
        
        self.best_params = study.best_params
        self.best_score = study.best_value
        
        print(f"\nâœ… Best accuracy: {self.best_score:.4f}")
        print(f"âœ… Best params: {self.best_params}")
        
        return self.best_params, self.best_score
    
    def save_best_params(self, filepath: str = "logic/ml_model_files/best_params.json"):
        """LÆ°u best params vÃ o file."""
        import json
        from pathlib import Path
        
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        data = {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'tuning_date': datetime.now().isoformat()
        }
        
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"âœ… ÄÃ£ lÆ°u best params vÃ o {filepath}")

# Usage
def run_hyperparameter_tuning(X, y, n_trials=100):
    """Cháº¡y hyperparameter tuning."""
    print(f"ðŸ” Báº¯t Ä‘áº§u tuning vá»›i {n_trials} trials...")
    
    tuner = HyperparameterTuner(X, y)
    best_params, best_score = tuner.tune(n_trials=n_trials)
    tuner.save_best_params()
    
    return best_params, best_score

# ThÃªm vÃ o UI
def on_tune_hyperparameters_click():
    """Handler cho nÃºt Tune Hyperparameters trong UI."""
    # Load data
    all_data_ai, _ = load_data_ai_from_db()
    daily_predictions = get_daily_bridge_predictions(all_data_ai)
    precomputed_features = prepare_features_for_ml(all_data_ai)
    
    # Prepare training data
    X, y = prepare_training_data(all_data_ai, daily_predictions, precomputed_features)
    
    # Run tuning
    best_params, best_score = run_hyperparameter_tuning(X, y, n_trials=50)
    
    # Show results
    messagebox.showinfo(
        "Tuning Complete",
        f"Best Accuracy: {best_score:.4f}\n\n"
        f"Best Params:\n{json.dumps(best_params, indent=2)}\n\n"
        f"Báº¡n cÃ³ muá»‘n train láº¡i model vá»›i params nÃ y khÃ´ng?"
    )
```

**Checklist:**
- [ ] Install Optuna
- [ ] Implement `HyperparameterTuner` class
- [ ] ThÃªm nÃºt "Tune Hyperparameters" trong UI (`ui_tuner.py`)
- [ ] Test tuning vá»›i small dataset
- [ ] Run full tuning (cÃ³ thá»ƒ máº¥t vÃ i giá»)
- [ ] Update config.json vá»›i best params

**Deliverables:**
- [ ] HyperparameterTuner working
- [ ] UI Ä‘á»ƒ trigger tuning
- [ ] Best params saved
- [ ] Model accuracy tÄƒng 5-10%

---

## ðŸš€ Week 7-8: DevOps & Production Ready

### âœ… Task 11: Logging System
**Thá»i gian:** 2 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** Backend Dev  
**Äá»™ Æ°u tiÃªn:** ðŸŸ¡ Trung bÃ¬nh

**Chi tiáº¿t:**
```python
# Setup logging
import logging
from logging.handlers import RotatingFileHandler
from pathlib import Path

def setup_logging():
    """Setup logging system."""
    # Create logs directory
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # Configure root logger
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    
    # Console handler (INFO level)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    
    # File handler (DEBUG level) vá»›i rotation
    file_handler = RotatingFileHandler(
        log_dir / "app.log",
        maxBytes=10 * 1024 * 1024,  # 10 MB
        backupCount=5
    )
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)
    
    # Error file handler (ERROR level)
    error_handler = RotatingFileHandler(
        log_dir / "error.log",
        maxBytes=10 * 1024 * 1024,  # 10 MB
        backupCount=5
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(file_formatter)
    
    # Add handlers
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    logger.addHandler(error_handler)
    
    return logger

# Usage trong code
# Replace print() vá»›i logger
# HIá»†N Táº I (SAI)
print("Äang táº£i dá»¯ liá»‡u...")  # âŒ

# Má»šI (ÄÃšNG)
logger = logging.getLogger(__name__)
logger.info("Äang táº£i dá»¯ liá»‡u...")  # âœ…
logger.debug(f"Chi tiáº¿t: {data}")
logger.warning("Cáº£nh bÃ¡o: dá»¯ liá»‡u cÃ³ thá»ƒ khÃ´ng Ä‘áº§y Ä‘á»§")
logger.error("Lá»—i khi táº£i dá»¯ liá»‡u", exc_info=True)
```

**Checklist:**
- [ ] Implement `setup_logging()` function
- [ ] Replace táº¥t cáº£ `print()` vá»›i `logger.info()`, `logger.debug()`, etc.
- [ ] Add try-except vá»›i logging.error() cho cÃ¡c functions quan trá»ng
- [ ] Test log rotation
- [ ] Add logs/ vÃ o .gitignore

**Deliverables:**
- [ ] Logging system working
- [ ] No more print() statements
- [ ] Logs rotating properly

---

### âœ… Task 12: CI/CD vá»›i GitHub Actions
**Thá»i gian:** 2 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** DevOps  
**Äá»™ Æ°u tiÃªn:** ðŸŸ¡ Trung bÃ¬nh

**Chi tiáº¿t:**
```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run linting
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=100 --statistics
    
    - name: Check code formatting
      run: |
        black --check .
        isort --check-only .
    
    - name: Run type checking
      run: |
        mypy logic/ --ignore-missing-imports
    
    - name: Run tests
      run: |
        pytest --cov=logic --cov-report=xml --cov-report=html
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

# .github/workflows/release.yml
name: Release

on:
  push:
    tags:
      - 'v*'

jobs:
  build:
    runs-on: windows-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pyinstaller
    
    - name: Build executable
      run: |
        pyinstaller main_app.spec
    
    - name: Create Release
      uses: softprops/action-gh-release@v1
      with:
        files: dist/main_app.exe
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

**Checklist:**
- [ ] Create `.github/workflows/ci.yml`
- [ ] Create `.github/workflows/release.yml`
- [ ] Setup Codecov account
- [ ] Test CI pipeline
- [ ] Add CI badge to README

**Deliverables:**
- [ ] CI pipeline running on every push
- [ ] Automated releases on tags
- [ ] Code coverage badge in README

---

### âœ… Task 13: Documentation vá»›i Sphinx
**Thá»i gian:** 2 ngÃ y  
**NgÆ°á»i thá»±c hiá»‡n:** Tech Writer / Dev  
**Äá»™ Æ°u tiÃªn:** ðŸŸ¢ Tháº¥p

**Chi tiáº¿t:**
```bash
# Install Sphinx
pip install sphinx sphinx-rtd-theme

# Setup Sphinx
mkdir docs
cd docs
sphinx-quickstart

# Configure
# Edit docs/conf.py
```

```python
# docs/conf.py
import os
import sys
sys.path.insert(0, os.path.abspath('..'))

project = 'Há»‡ Thá»‘ng PhÃ¢n TÃ­ch Xá»• Sá»‘'
copyright = '2026'
author = 'Your Team'

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.napoleon',  # Support Google/NumPy docstrings
    'sphinx.ext.viewcode',
]

html_theme = 'sphinx_rtd_theme'
```

```bash
# Generate documentation
cd docs
sphinx-apidoc -o . ../logic
make html

# Documentation sáº½ á»Ÿ docs/_build/html/index.html
```

**Checklist:**
- [ ] Install Sphinx
- [ ] Setup Sphinx project
- [ ] Generate API documentation
- [ ] Write user guide
- [ ] Deploy docs to GitHub Pages

**Deliverables:**
- [ ] API documentation generated
- [ ] User guide written
- [ ] Docs hosted on GitHub Pages

---

## ðŸ“Š Success Metrics & KPIs

### Metrics sau Week 2 (Quick Wins)
- [ ] âœ… Test coverage: 0% â†’ 70%+
- [ ] âœ… Build time: giáº£m 30%+
- [ ] âœ… Query time: giáº£m 50%+
- [ ] âœ… Zero flake8 warnings
- [ ] âœ… All functions cÃ³ type hints vÃ  docstrings

### Metrics sau Week 4 (Architecture Refactoring)
- [ ] âœ… ml_model.py khÃ´ng import bridges/analytics
- [ ] âœ… All modules dÃ¹ng Dependency Injection
- [ ] âœ… Test coverage: 70% â†’ 80%+

### Metrics sau Week 6 (AI & Monitoring)
- [ ] âœ… Model accuracy tÄƒng 5-10%
- [ ] âœ… Model versioning working
- [ ] âœ… Daily metrics calculation
- [ ] âœ… Metrics dashboard trong UI

### Metrics sau Week 8 (DevOps)
- [ ] âœ… CI pipeline running
- [ ] âœ… Automated deployment
- [ ] âœ… Logging system working
- [ ] âœ… API documentation complete

---

## ðŸŽ¯ Deliverables Summary

### Week 1-2
- [ ] 50+ unit tests (coverage 70%+)
- [ ] Caching implementation
- [ ] Code formatted vá»›i black
- [ ] Type hints & docstrings added

### Week 3-4
- [ ] Phá»¥ thuá»™c chÃ©o Ä‘Æ°á»£c giáº£i quyáº¿t
- [ ] Dependency Injection implemented
- [ ] Database queries optimized

### Week 5-6
- [ ] Model versioning system
- [ ] Model monitoring dashboard
- [ ] Hyperparameter tuning vá»›i Optuna
- [ ] Model accuracy tÄƒng 5-10%

### Week 7-8
- [ ] Logging system
- [ ] CI/CD pipeline
- [ ] API documentation
- [ ] Production-ready system

---

## ðŸš¨ Risks & Mitigation

### Risk 1: Breaking Changes
**Risk:** Refactoring cÃ³ thá»ƒ break existing functionality  
**Mitigation:**
- Viáº¿t tests trÆ°á»›c khi refactor
- Incremental changes, test sau má»—i change
- Keep backup cá»§a code cÅ©

### Risk 2: Performance Regression
**Risk:** ThÃªm logging/monitoring cÃ³ thá»ƒ lÃ m cháº­m há»‡ thá»‘ng  
**Mitigation:**
- Benchmark trÆ°á»›c vÃ  sau má»—i change
- Optimize logging (async logging)
- Profile code Ä‘á»ƒ tÃ¬m bottlenecks

### Risk 3: Data Loss
**Risk:** Migration database cÃ³ thá»ƒ máº¥t dá»¯ liá»‡u  
**Mitigation:**
- Backup database trÆ°á»›c khi migrate
- Test migration trÃªn copy cá»§a database
- CÃ³ rollback plan

---

## âœ… Sign-off

**Project Manager:** __________________  
**Tech Lead:** __________________  
**QA Lead:** __________________  

**NgÃ y phÃª duyá»‡t:** __________________

---

**TÃ i liá»‡u liÃªn quan:**
- `SYSTEM_EVALUATION.md` - ÄÃ¡nh giÃ¡ chi tiáº¿t há»‡ thá»‘ng
- `README.md` - HÆ°á»›ng dáº«n sá»­ dá»¥ng
- `Káº¿ Hoáº¡ch NÃ¢ng Cáº¥p Há»‡ Thá»‘ng PhÃ¢n TÃ­ch Xá»• Sá»‘ (V7.0)K.txt` - Káº¿ hoáº¡ch nÃ¢ng cáº¥p gá»‘c
